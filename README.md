
# Create AI Images of Yourself: A Practical Guide for Beginners

## What You Will Learn and Do in This Guide: The Steps to Create AI Art with Your Face
While trying to maximize the appeal of my LinkedIn profile to gain internships, I noticed a key component missing on my profile---an appealing profile picture. However, not being able to afford a professional photographer, I looked to my technical toolset for help.

With the advent and exponential increase in popularity of images generated by artificial intelligence, these powerful tools have become more accessible than ever; with it, so has the importance of staying up to date on the latest technology and utilizing it. 

Most people are likely familiar with existing AI image creation tools such as Midjourney or DALLE-2, however, this instructional manual seeks to teach you not only how to generate pictures of yourself in AI art, but also do it for completely free. Whether you choose to use this to create a new LinkedIn profile photo or envision yourself in fantasy worlds, the exposure to the behind-the-scenes AI image generation given from this guide will be invaluable to those interested in the subject. 

## Meet Dream Booth and Stable Diffusion: The Tools You Will Use to Create AI Art with Your Face

To create our own AI images, we will be utilizing multiple open-sourced tools, namely Stable Diffusion and a derivative plugin for Stable Diffusion known as DreamBooth. Stable Diffusion is an open-sourced latent diffusion text-to-image model which can generate, similar to popular tools, realistic and detailed images.

A latent diffusion model is trained by first giving the model clear images and telling it to add noise and static over it; the model is then instructed to remove the noise it added. Think of it as a sculptor learning how to turn sculptures into a normal block of clay---and then chisel away from the block of clay again, until he gradually reveals the effigy from scratch.

The DreamBooth extension we will be using is a fine-tuned model which allows you to embed a token of your likeness into an existing model, such as StableDiffusion, using a very small amount of training images.

Lastly, training models is a very computationally intensive task, which is why we will be utilizing Google Colab, a popular cloud-based service that gives you free access to a graphical processing unit (GPU) which is needed for creating our model. 

## Setting Up Our Environment: How to Access the Website and Install the Requirements

Firstly, access the Jupyter Notebook containing the code you will need to run on [this Google Colab notebook](https://colab.research.google.com/github/ShivamShrirao/diffusers/blob/main/examples/dreambooth/DreamBooth_Stable_Diffusion.ipynb#scrollTo=jjcSXTp-u-Eg). We will be using this code provided by ShivamShrirao on GitHub to streamline our process and make it beginner-friendly.

Upon opening the notebook you should see the following:
![Notebook image](https://i.ibb.co/RjPwSzF/Capture.png)

Luckily, this contains all the code you need, and your *environment* is set up!

## Training Your Model: How to Teach the AI to Recognize Your Face

To begin, run the first cell block by clicking on the play button
![enter image description here](https://i.ibb.co/sKDHM02/image.png)

Continue running each cell and following its instructions (where applicable) until you get to the one titled *Start Training*. At the time of writing this, the cell titled *Login to HuggingFace* is no longer required and you can either skip it or enter placeholder text; if you encounter errors, this may have been changed in the future, and you should follow the instructions on the cell to login to HuggingFace.

The *Start Training* cell is where we will input a token that the model will identify as us. To put it simply, a token is a word that the model associates with a certain object or quality. For example, if you feed the model the input of *car,* the model will output images of a car; in this scenario, the word *car* is a token that the model remembers contains features such as wheels, windows, etc. 

To generate images containing our face, we will need to teach the model a token that it associates with our face. To do this we will use something called a *rare token,* which is essentially a word so uncommon that the model has very few if any, attributes attached to it; for example, the string "ohwx" is considered a *rare token* because it means nothing--its simply random jargon. Because the model doesn't have any attributes associated with a rare token, we can map the features of our face to it. 

![enter image description here](https://i.ibb.co/1mC4cWm/Capture.png)

Since we are training the model to create images of a person and not a dog, we comment out the code for the dog and un-comment the code for a person. 

Edit the code displayed above by replacing it with the following:

```
concepts_list = [
	# {
	# "instance_prompt":  "photo of zwx person",
	# "class_prompt":  "photo of a zwx person",
	# "instance_data_dir":  "/content/data/zwx",
	#"class_data_dir":  "/content/data/dog"
	# },
	{
	"instance_prompt": "photo of ukj person",
	"class_prompt": "photo of a person",
	"instance_data_dir": "/content/data/ukj",
	"class_data_dir": "/content/data/person"
	}
]
```

In the code above *ukj* is our rare token.

 You might also be wondering about the difference between an instance and a class. For the model, an instance is a more specific type/occurrence of a class; for example, you are an instance of the class person. There are many person(s) in the world, but you are a separate and unique instance of a person. 

Once you have made this change, you can run the cell, which might take a couple of seconds to process.

Now, before we can continue training our model, we need to get pictures of our faces that we can show the model for training.

## Preparing Your Images: How to Choose and Edit Your Images for the Best Results 

Now, to ensure the model accurately learns your face---and only your face---there are several precautions we must take to preprocess the data, in this case, your face, before we train it.

Luckily, DreamBooth has clear instructions for what works best as training data. 

You will need between 13-20 images of yourself, each with differing backgrounds, clothes, and facial expressions. The model needs to be shown your face in different ways so that it can find the similarities in all of the images. For example, if you wear an orange t-shirt in every single image you show to the model, the model will think that the orange t-shirt is a physical part of what an image of you, or rather "ukj," must contain.

The poses and distance from the camera you have will also similarly impact the model. If you are looking to create photorealistic portrait photos, feed the model images close to your face; if you wanted the model to create full-body shots of you in different locations, feed the model images where your entire body is visible.

Showing too many images to your model, otherwise known as overtraining, can also produce bad results as the model starts to focus too much on the same thing. This can lead to disfigured faces in photos or photos that evoke the uncanny valley effect. 

Lastly, it's also recommended to crop our images to 512x512 pixels as that is primarily what the model is trained on and will output. You can do this for free through a website such as [Photopea](https://www.photopea.com/).

For our example of creating a realistic LinkedIn profile photo, take 13-20 photos where each image contains the following criteria:

 - Different clothing.
 - Different backgrounds.
 - Different facial expressions.
 - Image cropped to 512x512.
 - Image resembles a portrait photo, with the face visible from the shoulder up.

*Note: Since we want our images to be used for a LinkedIn profile photo, we are utilizing mostly close-up photos to train the model; because of this, if you use this model to generate images where your face is further away from the camera, your face will often come out disfigured.*

*If you intend to make a more well-rounded model which can output both close-ups and full-body images, you can input an alternate set of images that contain 13 close-up photos, 4 photos from the waist-up, and 3 full-body photos; though I have personally not tested this, threads online suggests this works just as well.*

Though I don't have a saved copy of the training images I used, here is an example of the range of images you should have I found online:
![enter image description here](https://i.ibb.co/zVhQbjP/image.png)
*Credit: [Matt Wolfe](https://www.youtube.com/@mreflow)*

Once you have a solid set of training data to use, you can proceed to the final few steps of training your very own AI model!

## How to Upload Your Images and Start the Training Process: How to Give Your Images to the AI and Let It Learn from Them

If all the above steps have been followed correctly, you should be located in the following cell:

![enter image description here](https://i.ibb.co/F47G8ny/image.png)

Run this cell or follow the instructions to drag and drop your training images into Google Colab---for those focused on privacy, Google automatically deletes images uploaded to an instance of a Colab notebook once the session is closed. 

Once the set of training images has been uploaded, the next cell should contain the following code:
![enter image description here](https://i.ibb.co/p1mLzV5/image.png)

Replace the code inside that cell with the following:
```
!python3 train_dreambooth.py \

--pretrained_model_name_or_path=$MODEL_NAME \

--pretrained_vae_name_or_path="stabilityai/sd-vae-ft-mse" \

--output_dir=$OUTPUT_DIR \

--revision="fp16" \

--with_prior_preservation --prior_loss_weight=1.0 \

--seed=1337 \

--resolution=512 \

--train_batch_size=1 \

--train_text_encoder \

--mixed_precision="fp16" \

--use_8bit_adam \

--gradient_accumulation_steps=1 \

--learning_rate=1e-6 \

--lr_scheduler="constant" \

--lr_warmup_steps=0 \

--num_class_images=12 \

--sample_batch_size=4 \

--max_train_steps=2000 \

--save_interval=2000 \

--save_sample_prompt="photo of ukj person" \

--concepts_list="concepts_list.json"

  

# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.

# `--save_sample_prompt` can be the same as `--instance_prompt` to generate intermediate samples (saved along with weights in the samples directory).
```
In the code above, we have made several changes, however, the most notable one is *max_train_steps,* which informs the model of how many training iterations it should perform on the same image. A greater number does not mean better results as this can lead to over-training the model on images, therefore a number between 1500-2000 works best. A general rule of thumb tends to be NUMBER_OF_PHOTOS * 100, so with 20 images, you get 2000 steps.

Once you have pasted in the edited code, you can go ahead and run the cell to start the mode's training!

This process will take around 30-40 minutes, however, keep the site open and scroll around or type something in the Colab notebook every 5-10 minutes to ensure you are not kicked out for being idle.

Once the model has finished training (indicated by a green checkmark on the play button, as well as in the output of the code cell), you can run the following code cells to generate preview images from the model!
![enter image description here](https://i.ibb.co/nCc7wPf/image.png)

## Generating Images with Your Face: How to Create AI Images with Your Face on Them

Now that the model has been created, you can use it to generate images from a text similar to how popular websites such as Midjourney and DALLE-2 work!

Before we start generating images, however, if your computer has a dedicated GPU with more than 4GB of VRAM, then you can run the cell titled *Convert weights to ckpt to use in web UIs like AUTOMATIC1111.* to download the model into your local machine. This will output a .ckpt ("checkpoint") file which allows you to generate images using compute from your local machine via tools such as AUTOMATIC1111, which is a visual user interface for generating images with Stable Diffusion. Feel free to skip this step if you don't think this applies to you.

Finally, we can use the last two cells to convert text into images. Before we do that, let me outline some helpful vocabulary:

 - A `prompt` is the text that you input into the model (ie. "photo of a car").
 - A `negative prompt` is input that the model avoids; for example, if your prompt is "photo of a red car", its negatives could be objects like a blue car, or a blue jet; negative prompts tell the model what to avoid while creating the output image.
 - A `seed` is an integer used to generate random noise, from which the final AI image is chiseled. Essentially, using the same seed with the same prompt will output the same image every time; use a different seed with the same prompt, and you will get a different image every time.
 - The `guidance_scale` tells the model how creative it should be. A number between 5-15 generates acceptable output; a higher number will lead the model to generate random noise since your telling it to be as creative as possible.
 - The `num_inference_steps` informs the model of how many times it should *chisel* away at a set of noises until it's decided an image has been made.

Now that you are aware of what each parameter does, you can generate images of yourself using any prompt you can imagine. For example, since we are attempting to create a LinkedIn profile photo, you may try something like:

	portrait photo of ukj person wearing a fancy suit
 Keep in mind that the model associates the token *ukj* with attributes of your face, so you must use that token to create images of yourself. You can experiment with different parameters by altering the values of the terms defined above until you get the output you seek.

## Conclusion
Congratulation! You have successfully reached the end of the guide and are now well-fit to generate AI art and images with your face! You have learned and done a lot in this guide, such as:
-   Preparing your images: You have chosen and edited quality images of yourself in different angles, backgrounds, and clothes, and resized them to help train the AI.
-   Learned how to use Juypter Notebooks: Google Colab uses a Jupyter Notebook environment to run code in cells, and you have successfully learned how to utilize it for our use case.
-   Training your model: You have trained a model to recognize your face using your images as well as understanding how aspects of the configuration affect training.
-   Generating images with your face: You have created realistic images with your face on them as well as experimented with different prompts, settings, and styles.

If this guide was interesting to you, consider looking into using more advanced features such as AUTOMATIC1111's WebUI and other StableDiffusion plugins. Thank you for reading and good luck making masterful AI images!